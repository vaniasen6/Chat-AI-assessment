{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU gradio langchain-cohere faiss-cpu python-dotenv langchain-community"
      ],
      "metadata": {
        "id": "jK96WkEm5-Zv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
        "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n",
        "from langchain_community.vectorstores import FAISS"
      ],
      "metadata": {
        "id": "2OmnsS116Jps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Config and Params"
      ],
      "metadata": {
        "id": "yfSCdA8SKL1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_TEMPRETURE = 0.1\n",
        "TOP_N_MATCH = 2 # Tune it as per number of examples in input data\n",
        "\n",
        "# Load fron .env file\n",
        "os.environ[\"COHERE_API_KEY\"] = \"xxxx\"\n",
        "\n",
        "DEFAULT_ANSWER = '\"Sorry, I can not provide answer to given question.\"'\n",
        "\n",
        "SYSTEM_PROMPT = f\"\"\"You are an enterprise grader customer support agent for Thoughtful AI.\n",
        "\n",
        "# TASK\n",
        "- You are provided some examples and a user question.\n",
        "- You need to answer user question only as per given example match.\n",
        "\n",
        "# RULES\n",
        "- If user question matches any example, provide the answer as it is mentioned in the example word by word.\n",
        "- If user question doesn't match any example, provide MANDATORILY {DEFAULT_ANSWER}.\n",
        "- Never answer any user question which does not match with provided examples.\n",
        "- If there is no match, only say {DEFAULT_ANSWER}.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "61IDVpPSDC75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zWXba2DNKJyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Models"
      ],
      "metadata": {
        "id": "qZA8gOzhKSLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_cohere import CohereEmbeddings, ChatCohere\n",
        "\n",
        "\n",
        "if \"COHERE_API_KEY\" not in os.environ:\n",
        "    raise ValueError(\"COHERE_API_KEY environment variable not set. Please set your Google API key.\")\n",
        "\n",
        "embeddings = CohereEmbeddings(\n",
        "    model=\"embed-english-v3.0\",\n",
        "    # input_type=\"search_query\"  # Optimized for question answering\n",
        ")\n",
        "\n",
        "llm = ChatCohere(\n",
        "    model=\"command-r\",\n",
        "    temperature=MODEL_TEMPRETURE,\n",
        "    max_tokens=1024\n",
        ")"
      ],
      "metadata": {
        "id": "EQx03V37C_q8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Raw Data"
      ],
      "metadata": {
        "id": "jjvBrtel6Rvd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "thoughtful_ai_qa = {\n",
        "    \"questions\": [\n",
        "        {\n",
        "            \"question\": \"What does the eligibility verification agent (EVA) do?\",\n",
        "            \"answer\": \"EVA automates the process of verifying a patient’s eligibility and benefits information in real-time, eliminating manual data entry errors and reducing claim rejections.\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"What does the claims processing agent (CAM) do?\",\n",
        "            \"answer\": \"CAM streamlines the submission and management of claims, improving accuracy, reducing manual intervention, and accelerating reimbursements.\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"How does the payment posting agent (PHIL) work?\",\n",
        "            \"answer\": \"PHIL automates the posting of payments to patient accounts, ensuring fast, accurate reconciliation of payments and reducing administrative burden.\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"Tell me about Thoughtful AI's Agents.\",\n",
        "            \"answer\": \"Thoughtful AI provides a suite of AI-powered automation agents designed to streamline healthcare processes. These include Eligibility Verification (EVA), Claims Processing (CAM), and Payment Posting (PHIL), among others.\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"What are the benefits of using Thoughtful AI's agents?\",\n",
        "            \"answer\": \"Using Thoughtful AI's Agents can significantly reduce administrative costs, improve operational efficiency, and reduce errors in critical processes like claims management and payment posting.\"\n",
        "        }\n",
        "    ]\n",
        "}"
      ],
      "metadata": {
        "id": "M0adLqgc6R4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "examples = [\n",
        "    {\"input\": q[\"question\"], \"output\": q[\"answer\"]}\n",
        "    for q in thoughtful_ai_qa[\"questions\"]\n",
        "]\n",
        "\n",
        "print(f\"Number of examples: {len(examples)}\")\n",
        "examples[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgFySd1R6Wkw",
        "outputId": "c2d4540b-9e1b-4e5e-e1ce-b2561abece58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of examples: 5\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What does the eligibility verification agent (EVA) do?',\n",
              " 'output': 'EVA automates the process of verifying a patient’s eligibility and benefits information in real-time, eliminating manual data entry errors and reducing claim rejections.'}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vectorization"
      ],
      "metadata": {
        "id": "ASH6QvI36t1L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Key Implementation Details:\n",
        "\n",
        "1. **Vector Similarity Search**:\n",
        "   - Uses `SemanticSimilarityExampleSelector` with FAISS vector store\n",
        "   - Embeds questions using OpenAI's text-embedding-3-small model\n",
        "   - Retrieves top 3 similar questions\n",
        "\n",
        "2. **Few-Shot Prompting**:\n",
        "   - Combines retrieved examples with the user's question\n",
        "   - Uses structured prompt template to guide the LLM"
      ],
      "metadata": {
        "id": "_FP-6prAJsfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_vDHeZzT6z4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
        "    examples,\n",
        "    embeddings,\n",
        "    FAISS,\n",
        "    k=TOP_N_MATCH  # Retrieve top N similar examples\n",
        ")\n"
      ],
      "metadata": {
        "id": "62-oTu7h6Wne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %% Create few-shot prompt template\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"input\", \"output\"],\n",
        "    template=\"Question: {input}\\nAnswer: {output}\"\n",
        ")"
      ],
      "metadata": {
        "id": "gzt2ldTO6WqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieve Top-N"
      ],
      "metadata": {
        "id": "X07qqWym-U0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_prompt = FewShotPromptTemplate(\n",
        "    example_selector=example_selector,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=\"You are a customer support agent for Thoughtful AI. Answer questions using these examples:\",\n",
        "    suffix=\"Question: {input}\\nAnswer:\",\n",
        "    input_variables=[\"input\"]\n",
        ")"
      ],
      "metadata": {
        "id": "gaqRJbN9-X7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chatbot Agent"
      ],
      "metadata": {
        "id": "i--W5Al8-iDU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. **Semantic Retrieval from Predefined Dataset**\n",
        "- The agent uses semantic similarity search to retrieve the most relevant answers from a hardcoded dataset about Thoughtful AI.\n",
        "- By leveraging vector embeddings and FAISS, it ensures that user queries are matched to the closest available knowledge, even if the wording differs.\n",
        "\n",
        "### 2. **Few-Shot Prompt Construction**\n",
        "- For each user question, the agent dynamically constructs a prompt using the top-matching examples from the dataset.\n",
        "- This few-shot approach improves answer accuracy and relevance, as the LLM is guided by real, contextually similar Q&A pairs.\n",
        "\n",
        "### 3. **LLM Fallback for Out-of-Domain Queries**\n",
        "- If the agent cannot find any relevant examples in the dataset, it automatically falls back to a generic LLM response.\n",
        "- This ensures that the chatbot remains conversational and helpful, even for queries outside the predefined scope.\n",
        "\n",
        "### 4. **Robust Error Handling**\n",
        "- The function is wrapped in a try-except block to catch and report any unexpected errors gracefully.\n",
        "- Handles missing or malformed data in the examples, skipping problematic entries without interrupting the user experience.\n",
        "- If no relevant examples are found, the agent still provides a meaningful response rather than failing silently.\n",
        "\n",
        "### 5. **Maintainability and Extensibility**\n",
        "- The function is modular and easy to update: new Q&A pairs can be added to the dataset without code changes.\n",
        "- The agent logic is separated from the UI layer, making it adaptable for CLI, web, or API-based interfaces.\n",
        "\n",
        "### 6. **Session History Support**\n",
        "- The function signature includes a `history` parameter, allowing for future enhancements such as context-aware multi-turn conversations."
      ],
      "metadata": {
        "id": "KOAz6_KBKnJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Chat Agent\n",
        "def chatbot_ai_agent(message: str, history: list = None) -> str:\n",
        "    try:\n",
        "        # Retrieve similar examples (modified to handle empty results)\n",
        "        similar_examples = example_selector.select_examples({\"input\": message}) or []\n",
        "\n",
        "        if not similar_examples:\n",
        "            return llm.invoke(f\"Answer about Thoughtful AI: {message}\").content\n",
        "\n",
        "        # Format examples with proper error handling\n",
        "        formatted_examples = []\n",
        "        for ex in similar_examples:\n",
        "            try:\n",
        "                formatted_examples.append(example_prompt.format(**ex))\n",
        "            except KeyError:\n",
        "                continue\n",
        "\n",
        "        # Build final prompt with fallback\n",
        "        final_prompt = f\"\"\"{SYSTEM_PROMPT}.\n",
        "\n",
        "        # Examples:\n",
        "        {''.join(formatted_examples)}\n",
        "\n",
        "        # User question:\n",
        "        {message}\n",
        "        Answer:\"\"\"\n",
        "\n",
        "        return llm.invoke(final_prompt).content\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Sorry, Something went wrong..!!\""
      ],
      "metadata": {
        "id": "QaKHX6eQ-kWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "3pRVv17kFrsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Case-1: Exact Match\n",
        "chatbot_ai_agent(message=\"What does the eligibility verification agent (EVA) do?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "b1MbVFHJD8aE",
        "outputId": "9a66ff4d-e943-44d9-8dd5-8093a709723b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'EVA automates the process of verifying a patient’s eligibility and benefits information in real-time, eliminating manual data entry errors and reducing claim rejections.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Case-2: Semantic Match\n",
        "chatbot_ai_agent(message=\"What does EVA do?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Kr-BDosbFVHw",
        "outputId": "76e77bb8-23fc-41b9-f520-fc41a6fdcd5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'EVA automates the process of verifying a patient’s eligibility and benefits information in real-time, eliminating manual data entry errors and reducing claim rejections.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Case-3: No Match\n",
        "chatbot_ai_agent(message=\"what is capital of USA?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "hPTU6t8NF4Ao",
        "outputId": "ea3ff081-52c6-4439-f14e-e2ceb069b217"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sorry, I can not provide answer to given question.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chatbot"
      ],
      "metadata": {
        "id": "45Hah-ZC-qAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr"
      ],
      "metadata": {
        "id": "k1JU9eXSDGDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradio interface with validated parameters\n",
        "demo = gr.ChatInterface(\n",
        "    fn=chatbot_ai_agent,\n",
        "    title=\"Thoughtful AI Support Agent\",\n",
        "    description=\"Ask about Thoughtful AI's products and solutions\",\n",
        "    examples=[q[\"question\"] for q in thoughtful_ai_qa[\"questions\"]],\n",
        "    cache_examples=True,\n",
        "    autofocus=True,\n",
        "    multimodal=False  # Explicitly disable multimodal input\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aru3me5SAz1r",
        "outputId": "d6c95626-20dd-4596-ec20-cd4e0d387edc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/chat_interface.py:338: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  self.chatbot = Chatbot(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MAkjF7S2HG3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final launch\n",
        "demo.launch(\n",
        "    share=True,\n",
        "    inline=True,\n",
        "    show_error=True,\n",
        "    debug=False  # Disable debug mode for production\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "id": "tIcl0HJtHKBz",
        "outputId": "f327a1ab-0a34-4ad5-9f43-93c178b63b29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caching examples at: '/content/.gradio/cached_examples/17'\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://66636bdafe9831883e.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://66636bdafe9831883e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VWFeheQsIOMQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}